#'@title Evaluates power for model matrix with a Monte Carlo simulation
#'
#'@description Evaluates design, given a run matrix, with a monte carlo simulation and returns
#'a data frame of parameter and effect powers.
#'
#'@param RunMatrix The run matrix of the design.
#'@param model The model used in the evaluation.
#'@param alpha The type-I error.
#'@param nsim The number of simulations.
#'@param glmfamily String indicating the family of distribution for the glm function
#'(e.g. "gaussian", "binomial", "poisson")
#'@param rfunction Random number generator function for the response variable. Should be a function of the form f(X,b,delta), where X is the
#'model matrix,b is a vector of the anticipated coefficients, and delta is a vector of noise due to blocking (one
#'element for each row in the run matrix). Typically something like rnorm(nrow(X), mean = X * b + delta, sd = 1).
#'
#'@param anticoef The anticipated coefficients for calculating the power. If missing, coefficients
#'will be automatically generated based on the delta argument.
#'
#'@param blocknoise Vector of noise levels (in standard deviations) for each block, one element per blocking level. See examples for details.
#'@param delta The signal-to-noise ratio. Default 2. This specifies the difference between the high
#'and low levels. If you do not specify anticoef, the anticipated coefficients will be half of delta.
#'@param conservative Default FALSE. Specifies whether default method for generating
#'anticipated coefficents should be conservative or not. TRUE will give the most conservative
#'estimate of power by setting all but one level in a categorical factor's anticipated coefficients
#'to zero.
#'@param contrasts The contrasts to use for categorical factors. Defaults to contr.sum.
#'@param parallel Default FALSE. If TRUE, uses all cores available to speed up computation.
#'@return A data frame consisting of the parameters and their powers. The parameter estimates from the simulations are
#'stored in the 'estimates' attribute.
#'@import foreach doParallel plyr
#'@export
#'@examples #We first generate a full factorial design using expand.grid:
#'factorialcoffee = expand.grid(cost=c(-1, 1),
#'                              type=as.factor(c("Kona", "Colombian", "Ethiopian", "Sumatra")),
#'                              size=as.factor(c("Short", "Grande", "Venti")))
#'
#'#And then generate the 21-run D-optimal design using gen_design.
#'
#'designcoffee = gen_design(factorialcoffee, model=~cost + type + size, trials=21, optimality="D")
#'
#'#To evaluate this design using a normal approximation, we just use eval_design
#'#(here using the default settings for contrasts, delta, and the anticipated coefficients):
#'
#'eval_design(RunMatrix=designcoffee, model=~cost + type + size, 0.05)
#'
#'#We want to evaluate this design with a Monte Carlo approach. In this case, we need
#'#to create a function that generates random numbers based on our run matrix X and
#'#our anticipated coefficients (b).
#'
#'rgen = function(X,b, delta) {
#'  return(rnorm(n=nrow(X), mean = X %*% b + delta, sd = 1))
#'}
#'
#'#Here we generate our nrow(X) random numbers from a population with a mean that varies depending
#'#on the design (and is set by multiplying the run matrix X with the anticipated coefficients
#'#vector b), and has a standard deviation of one. To evaluate this, we enter the same information
#'#used in eval_design, with the addition of the number of simulations "nsim", the distribution
#'#family used in fitting for the glm "glmfamily", the custom random generation function "rfunction",
#'#and whether or not we want the computation to be done with all the cores available "parallel".
#'
#'eval_design_mc(RunMatrix=designcoffee, model=~cost + type + size, alpha=0.05,
#'               nsim=100, glmfamily="gaussian", rfunction=rgen)
#'
#'#We see here we generate approximately the same parameter powers as we do
#'#using the normal approximation in eval_design. Like eval_design, we can also change
#'#delta to produce a different signal-to-noise ratio:
#'
#'eval_design_mc(RunMatrix=designcoffee, model=~cost + type + size, alpha=0.05,
#'               nsim=100, glmfamily="gaussian", rfunction=rgen, delta=1)
#'
#'#However, we could also specify this using a different random generator function by
#'#doubling the standard deviation of the population we are drawing from:
#'
#'rgensnr = function(X,b, delta) {
#'  return(rnorm(n=nrow(X), mean = X %*% b + delta, sd = 2))
#'}
#'
#'eval_design_mc(RunMatrix=designcoffee, model=~cost + type + size, alpha=0.05,
#'               nsim=100, glmfamily="gaussian", rfunction=rgensnr)
#'
#'#Both methods provide the same end result.
#'
#'#Like eval_design, we can also evaluate the design with a different model than
#'#the one that generated the design.
#'eval_design_mc(RunMatrix=designcoffee, model=~cost + type, alpha=0.05,
#'               nsim=100, glmfamily="gaussian", rfunction=rgen)
#'
#'#Here we evaluate the design using conservative anticipated coefficients:
#'eval_design_mc(RunMatrix=designcoffee, model=~cost + type + size, 0.05,
#'               nsim=100, glmfamily="gaussian", rfunction=rgen, conservative=TRUE)
#'
#'#And here it is evaluated with higher order effects included:
#'eval_design_mc(RunMatrix=designcoffee,model=~cost + type + size + cost*type, 0.05,
#'               nsim=100,glmfamily="gaussian",rfunction=rgen)
#'
#'#We can also set "parallel=TRUE" to turn use all the cores available to speed up
#'#computation.
#'\dontrun{eval_design_mc(RunMatrix=designcoffee,model=~cost + type + size, 0.05,
#'               nsim=100,glmfamily="gaussian",rfunction=rgen,parallel=TRUE)}
#'
#'#We can also evaluate split-plot designs. First, let us generate the split-plot design:
#'
#'vhtc = expand.grid(Store=as.factor(c("A","B")))
#'htc = expand.grid(Temp = c(1,-1))
#'
#'vhtcdesign = gen_design(factorial=vhtc, model=~Store, trials=6)
#'htcdesign = gen_design(factorial=htc, model=~Temp, trials=18, splitplotdesign=vhtcdesign, splitplotsizes=rep(3,6))
#'splitplotdesign = gen_design(factorial=factorialcoffee, model=~cost+type+size+size, trials=54,
#'                             splitplotdesign=htcdesign, splitplotsizes=rep(3,18))
#'
#'#Each block has an additional noise term associated with it in addition to the normal error term.
#'#This is specified by an additional random generating function and a vector specifying the input for
#'#each split-plot level. This function is stating that there is an addition gaussian noise term with each
#'#block, and the vector is stating it has a standard deviation of one for each level. This is equivalent to
#'#a variance ratio of one between the whole plots and the sub-plots.
#'#See the accompanying paper _____ for further technical details.
#'
#'
#'blockvector = c(1,1)
#'
#'#Evaluate the design. Note the decreased power for the blocking factors. If
#'eval_design_mc(RunMatrix=splitplotdesign, model=~Store+Temp+cost+type+size, alpha=0.05,
#'               nsim=100, glmfamily="gaussian", rfunction=rgen,
#'               blocknoise = blockvector)

#'
#'
#'#We can also use this method to evaluate designs that cannot be easily
#'#evaluated using normal approximations. Here, we evaluate a design and see
#'#if we can detect the difference between each factor changing whether an event
#'#70% of the time or 90% of the time.
#'
#'factorialbinom = expand.grid(a=c(-1,1),b=c(-1,1))
#'designbinom = gen_design(factorialbinom,model=~a+b,trials=90,optimality="D",repeats=100)
#'
#'#Here our random binomial generator simulates a response based on the resulting
#'#probability from of all the columns in one row influencing the result.
#'
#'rgenbinom = function(X, b, delta) {
#'  rbinom(n=nrow(X), size=1, prob = 1 / (1 + exp(-(X %*% b + delta))))
#'}
#'
#'#Plugging everything in, we now evaluate our model and obtain the binomial power.
#'#(the anticipated coefficients were determined empircally to set the
#'#high and low probabilities correctly for each factor)
#'
#'eval_design_mc(designbinom,~a+b,alpha=0.2,nsim=100,anticoef=c(1.5,0.7,0.7),
#'               glmfamily="binomial",rfunction=rgenbinom)
#'
#'#We can also use this method to determine power for poisson response variables.
#'#We design our test to detect if each factor changes the base rate of 0.2 by
#'#a factor of 2. We generate the design:
#'
#'factorialpois = expand.grid(a=as.numeric(c(-1,0,1)),b=c(-1,0,1))
#'designpois = gen_design(factorialpois,~a+b,trials=90,optimality="D",repeats=100)
#'
#'
#'#Here we return a random poisson number of events that vary depending
#'#on the rate in the design.
#'rrate = function(X, b, delta) {
#'  return(rpois(n=nrow(X),lambda=exp(X %*% b + delta)))
#'}
#'eval_design_mc(designpois,~a+b,0.2,nsim=100,glmfamily="poisson",rfunction=rrate,
#'               anticoef=c(log(0.2),log(2),log(2)))
#'#where the anticipated coefficients are chosen to set the base rate at 0.2
#'#(from the intercept) as well as how each factor changes the rate (a factor of 2, so log(2)).
#'#We see here we need about 90 test events to get accurately distinguish the three different
#'#rates in each factor to 90% power.
eval_design_mc = function(RunMatrix, model, alpha, nsim, glmfamily, rfunction,
                          blocknoise = NULL, anticoef, delta=2,
                          conservative=FALSE, contrasts=contr.sum, parallel=FALSE) {

  #---------- Generating model matrix ----------#
  #Remove columns from variables not used in the model
  RunMatrixReduced = reduceRunMatrix(RunMatrix,model)

  contrastslist = list()
  for(x in names(RunMatrixReduced[lapply(RunMatrixReduced, class) == "factor"])) {
    contrastslist[[x]] = contrasts
  }
  if(length(contrastslist) < 1) {
    contrastslist = NULL
  }

  #---------- Convert dot formula to terms -----#
  if((as.character(model)[2] == ".")) {
    model = as.formula(paste("~", paste(attr(RunMatrixReduced, "names"), collapse=" + "), sep=""))
  }

  ModelMatrix = model.matrix(model,RunMatrixReduced,contrasts.arg=contrastslist)
  #We'll need the parameter and effect names for output
  parameter_names = colnames(ModelMatrix)
  effect_names = c("(Intercept)", attr(terms(model), 'term.labels'))

  #-----Autogenerate Anticipated Coefficients---#
  if(missing(anticoef)) {
    anticoef = gen_anticoef(RunMatrixReduced, model, conservative=conservative)
  }
  if(length(anticoef) != dim(ModelMatrix)[2] && any(lapply(RunMatrixReduced,class)=="factor")) {
    stop("Wrong number of anticipated coefficients")
  }
  if(length(anticoef) != dim(ModelMatrix)[2] && !any(lapply(RunMatrixReduced,class)=="factor")) {
    anticoef = rep(1,dim(ModelMatrix)[2])
  }

  #-------------- Blocking errors --------------#
  blocking = FALSE

  #convert from our rownames construct to a run matrix indicating which block each run is in
  blocknames = rownames(RunMatrix)
  blocklist = strsplit(blocknames,".",fixed=TRUE)
  blocklist = lapply(blocklist, as.numeric) #so we can use block # for indexing later
  blockindicators = do.call(rbind,blocklist)  #matrix, indicates which block(s) each run is in
  levels_in_block = alply(blockindicators, 2, unique) #list of the levels in each block.
  #the last 'block' is just the run number, not really a block. We use alply to ensure the result is a list


  if(ncol(blockindicators) > 1) {
    if(is.null(blocknoise)) {
      warning("Warning: blocknoise argument missing. Blocking ignored.")
    } else {
    blocking = TRUE
    }
  }

  #-------Update formula with random blocks------#

  if(blocking) {
    randomeffects = c()
    for(i in 1:(ncol(blockindicators) - 1)) {
      RunMatrixReduced[paste("Block",i,sep="")] = blockindicators[, i]
      randomeffects = c(randomeffects, paste("( 1 | Block",i, " )", sep=""))
    }
    randomeffects = paste(randomeffects, collapse=" + ")
    blockform = paste("~. + ", randomeffects, sep="")
    #Adding random block variables to formula
    model = update.formula(model, blockform)
  }

  model_formula = update.formula(model, Y ~ .)
  RunMatrixReduced$Y = 1

  #---------------- Run Simulations ---------------#
  if(!parallel) {
    power_values = rep(0, length(parameter_names))
    effect_power_values = rep(0, length(effect_names))
    for (j in 1:nsim) {
      #simulate the data.
      noise_from_blocks = generate_block_noise(blockindicators, levels_in_block, blocknoise)
      RunMatrixReduced$Y = rfunction(ModelMatrix, anticoef * delta / 2, noise_from_blocks)

      if (blocking) {
        if(glmfamily == "gaussian") {
          fit = lme4::lmer(model_formula, data=RunMatrixReduced, contrasts = contrastslist)
        } else {
          fit = lme4::glmer(model_formula, data=RunMatrixReduced, family=glmfamily, contrasts = contrastslist)
        }
      } else {
        if (glmfamily == "gaussian") {
          fit = lm(model_formula, data=RunMatrixReduced, contrasts = contrastslist)
        } else {
          fit = glm(model_formula, family=glmfamily, data=RunMatrixReduced, contrasts = contrastslist)
        }
      }
      #determine whether beta[i] is significant. If so, increment nsignificant
      pvals = extractPvalues(fit)
      power_values[pvals < alpha] = power_values[pvals < alpha] + 1
      effect_power_values = effect_power_values + effectSignificance(pvals, alpha, attr(ModelMatrix, 'assign'))
    }
    #We are going to output a tidy data.frame with the results, so just append the effect powers
    #to the parameter powers. We'll use another column of that dataframe to label wether it is parameter
    #or effect power.
    power_values = c(power_values, effect_power_values)/nsim

  } else {
    cl <- parallel::makeCluster(parallel::detectCores())
    doParallel::registerDoParallel(cl, cores = parallel::detectCores())

    power_values = foreach::foreach (j = 1:nsim, .combine = "+", .packages = c("lme4")) %dopar% {
      power_values = rep(0, ncol(ModelMatrix))

      #simulate the data.
      noise_from_blocks = generate_block_noise(blockindicators, levels_in_block, blocknoise)
      RunMatrixReduced$Y = rfunction(ModelMatrix, anticoef * delta / 2, noise_from_blocks)

      if (blocking) {
        if(glmfamily == "gaussian") {
          fit = lme4::lmer(model_formula, data=RunMatrixReduced, contrasts = contrastslist)
        } else {
          fit = lme4::glmer(model_formula, data=RunMatrixReduced, family=glmfamily, contrasts = contrastslist)
        }
      } else {
        if (glmfamily == "gaussian") {
          fit = lm(model_formula, data=RunMatrixReduced, contrasts = contrastslist)
        } else {
          fit = glm(model_formula, family=glmfamily, data=RunMatrixReduced,contrasts = contrastslist)
        }
      }
      #determine whether beta[i] is significant. If so, increment nsignificant
      pvals = extractPvalues(fit)
      power_values[pvals < alpha] = 1
      effect_power_values = effectSignificance(pvals, alpha, attr(ModelMatrix, 'assign'))

      #We are going to output a tidy data.frame with the results, so just append the effect powers
      #to the parameter powers. We'll use another column of that dataframe to label wether it is parameter
      #or effect power.
      c(power_values, effect_power_values)
    }
    parallel::stopCluster(cl)
    power_values = power_values/nsim
  }
  #output the results (tidy data format)
  return(data.frame(parameters=c(parameter_names, effect_names),
                    type=c(rep("parameter.power.mc", length(parameter_names)),
                           rep("effect.power.mc", length(effect_names))),
                    power=power_values))
}
globalVariables('i')
